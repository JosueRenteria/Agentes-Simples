{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosueRenteria/Agentes-Simples/blob/master/Red%20Neuronal%20(Predicci%C3%B3n%20de%20Banco).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio de Redes Neuronales"
      ],
      "metadata": {
        "id": "uH-sigqbpjZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción del Problema"
      ],
      "metadata": {
        "id": "b1R0FZnN6tfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tiene un DataSet de clientes inscritos en un banco y el banco desea saber porque los clientes están abandonando el banco y porque la gente se está yendo. Todos los datos están guardados en el DataSet (.csv). Este problema ayuda si una persona es o no es (positivo o negativo, 1 o 0). Problema de Clasificación "
      ],
      "metadata": {
        "id": "ili_6dnw5HyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre procesamiento de Datos"
      ],
      "metadata": {
        "id": "CGD5j_8X50XY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importacion del DataSet"
      ],
      "metadata": {
        "id": "uwj4lNos90oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exportamos las librerias importantes para iniciar el programa."
      ],
      "metadata": {
        "id": "cVfr3XdX7gt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "z4q8Bk8_6PU2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexion a mi carpeta de Google Drive."
      ],
      "metadata": {
        "id": "cxdN4KPn-Qjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbjrhhnb-X6D",
        "outputId": "db8f247a-80d7-49fa-eb44-8e74e132f515"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexion al archivo que deseo ocupar (Datos del Banco)."
      ],
      "metadata": {
        "id": "_U54xCNy-la_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/Curso de Deep Learning de la A a la Z/datasets/Part 1 - Artificial Neural Networks (ANN)/Churn_Modelling.csv')\n",
        "# print(dataset)"
      ],
      "metadata": {
        "id": "0cZqTagn-swo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraccion de las filas y columnas."
      ],
      "metadata": {
        "id": "12OjUlvw_Rnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de Caracteristicas.\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "# Vector de la variable dependiente.\n",
        "y = dataset.iloc[:, 13].values"
      ],
      "metadata": {
        "id": "ITbVgWkNAE-c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codificar datos categóricos"
      ],
      "metadata": {
        "id": "N1AeU6iCBgZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos la libreria para el maching learning."
      ],
      "metadata": {
        "id": "QlqZd0T0BlBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "XtB7EJpVBlKX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codificamos las variables categoricas que no son numeros y se hacen variables dummy (0 o 1)."
      ],
      "metadata": {
        "id": "a48UxP8CBwBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificamos en variable Damit el apartado de \"Geografia\", lo pasa a numeros (0, 1, 2, etc. Dependiendo el pais).\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
        "\n",
        "# Codificamos en variable Damit el apartado de \"Genero\" (0 o 1 dependiendo el genero).\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
      ],
      "metadata": {
        "id": "Zh4bOvSbBwyH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las librerias de Maching Learning para transformar todas las variables a unas mismas y no tener categoricas y numericas."
      ],
      "metadata": {
        "id": "tar57c7sDUTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "metadata": {
        "id": "CF9UlDr1DUfW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos en la columna de paises que se separen en mas filas dependiendo el pais."
      ],
      "metadata": {
        "id": "K1hrQLrNDdNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Una lista de transformaciones.\n",
        "transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"Churn_Modelling\",        # Un nombre de la transformación\n",
        "         OneHotEncoder(categories='auto'), # La clase a la que transformar\n",
        "         [1]            # Especifica que se transformará la segunda columna (índice 1) del conjunto de datos original\n",
        "         )\n",
        "    ], remainder='passthrough' # Las columnas que no se especificaron en la lista de transformaciones se mantendrán sin cambios.\n",
        ")\n",
        "\n",
        "X = transformer.fit_transform(X)\n",
        "X = X[:, 1:]"
      ],
      "metadata": {
        "id": "K7z61TpGDdX5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividir el data set en conjunto de entrenamiento y conjunto de testing"
      ],
      "metadata": {
        "id": "q-EZyLFFF8PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos libreria para dividir los conjuntos de entrenamiento y test."
      ],
      "metadata": {
        "id": "knjTzoNIF-kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "WllCa5kmF-vA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos las variables de entrenamiento y las variables que se ocuparan para hacer el test."
      ],
      "metadata": {
        "id": "o0fefOahGNAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "D1JiRdDGGNK-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Escalado de Variables."
      ],
      "metadata": {
        "id": "IgOsQfKJGpES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos la libreria para el escalado."
      ],
      "metadata": {
        "id": "Wj_-PrgeHR3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "dDkqK418HSLr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza un escalado de variables cuando las escalas de los numeros son diferentes (En este caso tenemos muchas variables en rango de 0 o 1, numericas, precios, etc.). Aqui puede que tengamos variables negativas y que ya no entendamos muy bien (Variables Normalizadas)."
      ],
      "metadata": {
        "id": "L2RgWbaQGptq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaramos el metodo para el escalado\n",
        "sc_X = StandardScaler()\n",
        "\n",
        "# Ponemos el escalado a las variables de entrenamiento X. Calculamos el cambio de escala y lo aplicamos. \n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "# Ponemos el escalado a las variables de test X. Aqui solo se hace el transform ya que en la anterior linea se calculo.\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "metadata": {
        "id": "y1bfPg4uGp3W"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2 - Construir la RNA"
      ],
      "metadata": {
        "id": "d014JTIkIFY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importamos las librerias para trabajar nuestra Red Neuronal"
      ],
      "metadata": {
        "id": "9Cr0nc77IHqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importacion de Keras y librerías adicionales."
      ],
      "metadata": {
        "id": "TlMLzgdHJpfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout"
      ],
      "metadata": {
        "id": "FCepLENcJoCM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos y inicializamos el modelo de clasificacion (Red Neuronal Artificial)."
      ],
      "metadata": {
        "id": "QDWXynjeKFS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializacion del Modelo de la RNA (con Sequential)\n",
        "classifier = Sequential()"
      ],
      "metadata": {
        "id": "sb9HVi0HKFft"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la Capas de manera sucesiva con una secuencia de capas. Y añadimos las clases que tendra esta."
      ],
      "metadata": {
        "id": "EymAXbdDLQmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   units: especifica el número de neuronas en la capa (en este caso, 6).\n",
        "*   kernel_initializer: especifica cómo se inicializan los pesos de las conexiones entre las neuronas de la capa (en este caso, de forma uniforme).\n",
        "*   activation: especifica la función de activación a utilizar en las neuronas de la capa (en este caso, ReLU).\n",
        "*   input_dim: especifica el número de variables de entrada en la capa (en este caso, 11).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eqxg0ZZ3MEP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Añadir las capas de entrada y primera capa oculta\n",
        "classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  \n",
        "                     activation = \"relu\", input_dim = 12))\n",
        "\n",
        "# Añadir la segunda capa oculta\n",
        "classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  \n",
        "                     activation = \"relu\"))\n",
        "\n",
        "# Añadir la capa de salida\n",
        "classifier.add(Dense(units = 1, kernel_initializer = \"uniform\",  \n",
        "                     activation = \"sigmoid\"))\n",
        "# Compilar la RNA\n",
        "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "# Ajustamos la RNA al Conjunto de Entrenamiento\n",
        "classifier.fit(X_train, y_train,  batch_size = 10, epochs = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "NJziuCMwLQuc",
        "outputId": "c2874de2-9cf0-4e28-bd3a-85476c39e72a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ed0ac39034d1>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Ajustamos la RNA al Conjunto de Entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 11), found shape=(10, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos y configuramos el Modelo de la Red Neuronal"
      ],
      "metadata": {
        "id": "NpOKOKpYLpbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   optimizer: especifica el algoritmo de optimización a utilizar para minimizar la función de pérdida del modelo durante el entrenamiento. En este caso, se utiliza el algoritmo de optimización Adam (\"adam\").\n",
        "*   loss: especifica la función de pérdida que se utilizará para evaluar la precisión del modelo durante el entrenamiento. En este caso, se utiliza la función de pérdida binaria cruzada (\"binary_crossentropy\") porque se está trabajando en un problema de clasificación binaria.\n",
        "*   metrics: especifica la métrica a utilizar para evaluar el desempeño del modelo durante el entrenamiento. En este caso, se utiliza la métrica de precisión (\"accuracy\"), que mide la fracción de predicciones correctas del modelo."
      ],
      "metadata": {
        "id": "a_QkAojiS_Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "_iqrbYlMLppg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos la Red Neuronal."
      ],
      "metadata": {
        "id": "OZQYIzNhT5qi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "PaD66xdbT5zE",
        "outputId": "5e4db242-f719-46b5-c52c-10b4bd9563f7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-4631ca06651f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ajustamos la RNA al Conjunto de Entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 11), found shape=(10, 12)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}